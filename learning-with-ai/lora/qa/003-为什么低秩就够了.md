# 为什么低秩就够了?

## 核心观察:模型适配的内在维度很低

LoRA 的理论基础来自于一个重要发现:**预训练语言模型虽然参数巨大,但其"内在维度"(intrinsic dimension)很低**。

### 什么是内在维度?

内在维度是指:完成某个任务实际需要的**独立参数数量**。

一个直观的类比:
```
想象一个 1000 维的空间
但实际上所有有意义的点都落在一个 10 维的子空间中
那么这个问题的内在维度就是 10,而不是 1000
```

### 理论依据

**Li et al. (2018)** 的研究表明:
- 大型预训练模型可以投影到**很低维度的子空间**中,仍然能有效学习
- 这说明模型的参数空间有大量冗余

**Aghajanyan et al. (2020)** 进一步证明:
- 预训练语言模型有低"内在维度"
- 即使随机投影到更小的子空间,仍能高效学习

### LoRA 的假设

基于上述研究,LoRA 提出:**权重的更新 $\Delta W$ 也有低"内在秩"**

换句话说:
- 预训练模型 $W_0$ 已经学到了丰富的通用特征
- 适配到下游任务只需要调整**少数几个关键方向**
- 这些方向可以用**低秩矩阵**表示

---

## 数学视角:为什么低秩够用?

### 1. 矩阵的秩与信息量

回顾矩阵的秩的定义:
- 矩阵的秩 = 线性无关的行(列)的最大数量
- 低秩意味着矩阵可以用**少量独立向量的线性组合**表示

### 2. 低秩分解的含义

假设 $\Delta W \in \mathbb{R}^{d \times k}$ 秩为 $r$,那么可以分解为:

$$
\Delta W = BA
$$

其中 $B \in \mathbb{R}^{d \times r}$, $A \in \mathbb{R}^{r \times k}$

**几何解释**:
```
A: 将输入空间 ℝ^k 投影到低维"任务子空间" ℝ^r
B: 将低维特征映射回输出空间 ℝ^d
```

**意义**:
- 原本需要 $d \times k$ 个参数
- 现在只需要 $r(d+k)$ 个参数
- 当 $r \ll \min(d,k)$ 时,参数大幅减少

### 3. 任务适配的本质

从特征的角度理解:

**预训练阶段**:
- 模型学习了大量通用特征方向
- 这些特征编码在 $W_0$ 中

**微调阶段**:
- 不需要学习新的特征
- 只需要**重新组合/强化**某些已有特征
- 或者添加少量**任务特定**的特征方向

这种"特征重组"可以用低秩更新实现!

---

## 论文的实验证据

### 实验 1: 极小的秩就有效

论文在 GPT-3 175B 上的实验(Section 7.2, Table 6):

| 权重类型 | r=1 | r=2 | r=4 | r=8 | r=64 |
|---------|-----|-----|-----|-----|------|
| **WikiSQL (准确率%)** |
| 只有 $W_q$ | 68.8 | 69.6 | 70.5 | 70.4 | 70.0 |
| $W_q + W_v$ | 73.4 | 73.3 | **73.7** | 73.8 | 73.5 |
| $W_q+W_k+W_v+W_o$ | 74.1 | 73.7 | 74.0 | 74.0 | 73.9 |
| **MultiNLI (准确率%)** |
| 只有 $W_q$ | 90.7 | 90.9 | 91.1 | 90.7 | 90.7 |
| $W_q + W_v$ | 91.3 | 91.4 | 91.3 | **91.6** | 91.4 |
| $W_q+W_k+W_v+W_o$ | 91.2 | 91.7 | 91.7 | 91.5 | 91.4 |

**关键观察**:
1. **r=1 或 r=2 就接近最优性能**
2. **r 增大后性能提升很小甚至下降**
3. 说明任务适配确实只需要很少的特征方向

### 实验 2: 子空间相似性分析

论文做了一个巧妙的实验(Section 7.2):

**问题**: $r=8$ 和 $r=64$ 学到的特征方向有多大重叠?

**方法**: 对 $\Delta W$ 做 SVD 分解,比较不同 $r$ 的主要奇异向量的相似度

**结果** (Figure 3):
```
φ(Ar=8, Ar=64, i, j) 表示:
  Ar=8 的前 i 个奇异向量
  与 Ar=64 的前 j 个奇异向量
  的子空间相似度
```

**发现**:
- **第一主方向**(最重要的奇异向量)在 $r=8$ 和 $r=64$ 中**高度相似** (相似度 > 0.5)
- 其他方向相似度很低
- 说明:**只有少数方向是真正重要的,其他都是噪声**

**结论**:
> The top singular-vector directions of $A_{r=8}$ and $A_{r=64}$ are the most useful, while other directions potentially contain mostly random noises accumulated during training.

---

## 更深层的原因:LoRA 放大了什么?

### 实验 3: $\Delta W$ 与 $W$ 的关系

论文研究了 $\Delta W$ 与预训练权重 $W$ 的关系(Section 7.3, Table 7):

**方法**:
- 将 $W$ 投影到 $\Delta W$ 的子空间
- 计算投影的范数 $\|U^\top W V^\top\|_F$
- 其中 $U, V$ 是 $\Delta W$ 的左右奇异向量

**结果** (以 GPT-3 第 48 层 $W_q$ 为例):

| 投影方向 | r=4 | r=64 |
|---------|-----|------|
| $\Delta W_q$ 的方向 | **0.32** | **1.90** |
| $W_q$ 的前 r 个主方向 | 21.67 | 37.71 |
| 随机方向 | 0.02 | 0.33 |
| $\|W_q\|_F$ | 61.95 | 61.95 |
| $\|\Delta W_q\|_F$ | 6.91 | 3.57 |

**关键发现**:

1. **$\Delta W$ 不是重复 $W$ 的主要方向**
   - $W$ 的主方向投影: 21.67
   - $\Delta W$ 的方向投影: 0.32
   - 说明 $\Delta W$ 指向的是 $W$ **不强调**的方向

2. **放大因子巨大**
   - 计算: $\frac{\|\Delta W\|_F}{\|U^\top W V^\top\|_F} = \frac{6.91}{0.32} \approx 21.6$
   - 说明 $\Delta W$ 将某些**不明显的特征放大了 21 倍**!

3. **任务特定的特征增强**
   - 预训练时这些特征存在但不突出
   - 微调时通过低秩更新将它们**放大**
   - 使模型更关注任务相关的信息

### 直观理解

```
预训练模型 W:
  特征 A ████████████ (通用,重要)
  特征 B ██████ (通用,次要)
  特征 C ██ (任务特定,但被预训练忽略)
  特征 D █ (任务特定,但被预训练忽略)

LoRA 的 ΔW:
  特征 A  (不变,已经够好)
  特征 B  (不变)
  特征 C ████████████████████ (放大 20 倍!)
  特征 D ████████████ (放大 10 倍!)

最终 W + ΔW:
  特征 A ████████████ (保持)
  特征 B ██████ (保持)
  特征 C ████████████████████ (现在很重要!)
  特征 D ████████████ (现在也重要!)
```

---

## 为什么不是高秩?

### 高秩的问题

如果 $r$ 太大会怎样?

**实验观察**:
- $r=64$ 的性能**不如** $r=4$ 或 $r=8$
- 原因:高秩引入了太多**不必要的自由度**

**过拟合风险**:
- 低秩:约束了参数空间,正则化效果好
- 高秩:参数空间大,容易过拟合训练集

**噪声问题**:
- 如前面的子空间相似性实验所示
- 高秩的大部分方向是**训练噪声**,而非有用信号

### 低秩的优势

1. **强正则化**: 限制了模型复杂度
2. **更好的泛化**: 只学习最关键的特征
3. **计算高效**: 参数少,训练快
4. **理论优雅**: 符合"内在维度"假设

---

## 总结:为什么低秩就够了?

### 三个层面的理由

**理论层面**:
- 预训练模型有低内在维度
- 任务适配只需要调整少数关键方向
- 符合机器学习的"奥卡姆剃刀"原则

**实验层面**:
- r=1 或 r=2 就能达到接近最优性能
- 高 r 不仅无益,反而可能有害
- 不同 r 学到的主方向高度相似

**机制层面**:
- LoRA 不是学习新特征,而是**放大**已有的任务相关特征
- 这种"特征选择+放大"只需要少量参数
- 低秩矩阵恰好能高效表示这种操作

### 类比理解

把预训练模型想象成一个**多功能工具箱**:
- 里面有成千上万种工具(特征)
- 对于特定任务,你只需要**调整 2-4 个旋钮**
- 这 2-4 个旋钮就是秩 $r$
- 不需要重新制造所有工具,只需要**选择和调节**

---

## 补充:为什么正好是 1-8?

为什么不是 100? 不是 0.1?

**下界 (为什么不能更小)**:
- r=1: 只有一个自由方向,对某些复杂任务可能不够
- r<1: 无意义(秩至少为 1)

**上界 (为什么不能更大)**:
- r=64: 开始引入噪声,过拟合风险增加
- r=1000: 接近全秩,失去低秩的优势

**最优范围 (1-8)**:
- 刚好捕捉主要的任务相关方向
- 足够的表达能力,又不会过拟合
- 在计算效率和性能之间取得平衡

**任务相关**:
- 简单任务(如情感分析): r=1 或 r=2 足够
- 复杂任务(如翻译): 可能需要 r=4 或 r=8
- 但很少需要 r>16

---

这就是为什么低秩就够了!关键在于:**任务适配是特征重组,不是特征学习**。
