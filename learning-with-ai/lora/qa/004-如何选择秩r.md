# 如何选择秩 r?

## 快速答案

**对于大多数任务,r=4 或 r=8 是一个安全的选择。**

但最优的 r 取决于:
1. 任务的复杂度
2. 模型的大小
3. 可用的训练数据量
4. 参数预算

---

## 论文的建议

### GPT-3 175B 的实验结果

LoRA 论文在不同任务上测试了各种 r 值:

| 任务 | 最优 r | 性能 |
|-----|-------|------|
| WikiSQL | r=1 或 r=2 | 73.4% - 73.8% |
| MultiNLI | r=1 或 r=2 | 91.3% - 91.6% |
| SAMSum | r=2 | 最优 |

**观察**:
- 对于分类任务(MultiNLI),r=1 或 r=2 就足够
- 对于生成任务,可能需要稍大的 r
- 但很少需要 r > 8

### 不同模型大小的对比

论文在 GPT-2 上的实验(Table 18):

| r值 | 验证损失 | BLEU | 评价 |
|----|---------|------|------|
| 1  | 1.23    | 68.72 | 稍差 |
| 2  | 1.21    | 69.17 | 较好 |
| 4  | 1.18    | **70.38** | **最优** |
| 8  | 1.17    | 69.57 | 略降 |
| 16 | 1.16    | 69.61 | 略降 |
| 64 | 1.16    | 69.24 | 继续降 |

**结论**:
- GPT-2 Medium 的最优 r 在 4-8 之间
- r 太小:性能不足
- r 太大:引入噪声,性能下降

---

## 选择 r 的一般原则

### 原则 1: 从小开始

**建议**: 从 r=1 或 r=2 开始尝试

**理由**:
- 论文证明极低的秩通常就够了
- 小 r 训练更快,方便快速迭代
- 可以建立性能基线

**操作**:
```
步骤 1: 尝试 r=1, 记录性能
步骤 2: 尝试 r=2, 比较是否提升
步骤 3: 如果 r=2 明显更好,继续尝试 r=4
步骤 4: 如果性能不再提升,停止
```

### 原则 2: 考虑任务复杂度

**简单任务** (r=1 或 r=2):
- 二分类(情感分析、垃圾邮件检测)
- 简单的文本匹配
- 单一领域的适配

**中等任务** (r=4 或 r=8):
- 多分类(主题分类、意图识别)
- 命名实体识别
- 文本摘要
- 大多数NLU任务

**复杂任务** (r=8 或更大):
- 机器翻译(特别是低资源语言)
- 开放域对话生成
- 复杂推理任务
- 代码生成

**注意**: 即使是复杂任务,也很少需要 r > 16

### 原则 3: 根据模型大小调整

模型大小与最优 r 的关系:

| 模型大小 | 建议 r 范围 | 备注 |
|---------|-----------|------|
| < 1B 参数 | 2-8 | 小模型可能需要稍大的 r |
| 1B-10B | 2-8 | 标准范围 |
| 10B-100B | 1-4 | 大模型通常 r 更小就够 |
| > 100B | 1-2 | 超大模型 r=1 常常足够 |

**解释**:
- 更大的模型学到了更丰富的特征
- 适配时需要调整的维度反而更少
- GPT-3 175B 只需要 r=1 或 r=2

### 原则 4: 考虑数据量

训练数据量也影响 r 的选择:

**小数据集** (< 1000 样本):
- 推荐 r=1 或 r=2
- 避免过拟合
- 低秩提供强正则化

**中等数据集** (1k-100k):
- r=4 或 r=8 通常最优
- 有足够数据学习更多方向

**大数据集** (> 100k):
- 可以尝试 r=8 或更大
- 但提升通常有限

**论文证据** (Section F.3, Table 16):

| 数据量 | LoRA (r=8) | Full Fine-Tune | PrefixLayer |
|-------|-----------|---------------|------------|
| 100 样本 | **63.8%** | 60.2% | 48.3% |
| 1k 样本 | **85.6%** | 85.8% | 82.5% |
| 10k 样本 | **89.2%** | 88.9% | 85.9% |
| 392k 样本 | **91.7%** | 89.5% | 89.6% |

**观察**: LoRA 在小数据上尤其有效,低秩起到了正则化作用

---

## 选择 r 的实验方法

### 方法 1: 网格搜索

最直接但计算量大的方法:

```python
# 伪代码
r_values = [1, 2, 4, 8, 16]
results = {}

for r in r_values:
    model = create_lora_model(r=r)
    val_score = train_and_evaluate(model)
    results[r] = val_score

best_r = max(results, key=results.get)
```

**优点**: 找到真实最优值
**缺点**: 需要训练多次

### 方法 2: 渐进式搜索

更高效的策略:

```python
# 从 r=1 开始
r = 1
prev_score = 0

while r <= 16:
    score = train_and_evaluate(r=r)

    if score <= prev_score:
        # 性能不再提升,停止
        best_r = r // 2
        break

    prev_score = score
    r *= 2  # 翻倍增加
```

**优点**: 更快找到合适的 r
**缺点**: 可能错过局部最优

### 方法 3: 经验法则

基于论文结果的快速选择:

```python
def suggest_r(model_size, task_type, data_size):
    """
    model_size: 模型参数量 (单位: billion)
    task_type: 'simple' | 'medium' | 'complex'
    data_size: 训练样本数量
    """

    # 基础 r
    if model_size < 1:
        base_r = 4
    elif model_size < 10:
        base_r = 4
    elif model_size < 100:
        base_r = 2
    else:
        base_r = 1

    # 根据任务调整
    if task_type == 'simple':
        base_r = max(1, base_r // 2)
    elif task_type == 'complex':
        base_r = min(8, base_r * 2)

    # 根据数据量调整
    if data_size < 1000:
        base_r = max(1, base_r // 2)  # 小数据用更小的 r

    return base_r

# 示例
r = suggest_r(model_size=175, task_type='medium', data_size=50000)
print(f"建议 r = {r}")  # 输出: 建议 r = 2
```

---

## 参数预算约束

有时你有固定的参数预算,需要反推 r。

### 参数量计算

对于 Transformer,应用 LoRA 到 $W_q$ 和 $W_v$:

$$
|\Theta| = 2 \times L \times 2 \times d_{model} \times r = 4Ldr
$$

其中:
- $L$ = 层数
- $d_{model}$ = 模型维度
- $r$ = 秩

### 反推 r

如果有参数预算 $B$:

$$
r = \frac{B}{4Ld}
$$

**示例 (GPT-3 175B)**:
- $L = 96$, $d_{model} = 12288$
- 预算 $B = 5M$ 参数
- $r = \frac{5,000,000}{4 \times 96 \times 12288} \approx 1.06$
- 取 $r = 1$

### 在不同权重矩阵间分配 r

如果只适配一个矩阵:

$$
r_{single} = \frac{B}{2Ld}
$$

如果适配所有 4 个 attention 矩阵:

$$
r_{all} = \frac{B}{8Ld}
$$

**权衡**:
- 更多矩阵 + 小 r: 覆盖面广
- 更少矩阵 + 大 r: 单点深入

论文建议:**适配 2 个矩阵 ($W_q, W_v$) 平衡最好**

---

## 不同应用场景的 r 建议

### 场景 1: 快速原型开发

**目标**: 快速验证想法

**建议**: r=4
- 训练快速
- 性能通常够用
- 易于迭代

### 场景 2: 生产部署

**目标**: 最优性能

**建议**: 进行 r 搜索
- 在 [1, 2, 4, 8] 中选择
- 用验证集选最优
- 考虑性能-成本权衡

### 场景 3: 资源受限

**目标**: 极小参数量

**建议**: r=1 或 r=2
- 论文证明通常够用
- 参数量最小
- 可能需要更仔细的超参数调优

### 场景 4: 多任务学习

**目标**: 一个基座模型 + 多个任务

**建议**: 固定 r=4 或 r=8
- 所有任务用同一个 r
- 简化管理
- 任务切换更方便

### 场景 5: 持续学习

**目标**: 不断添加新任务

**建议**: 保守选择 r=8
- 为未来任务留有余地
- 避免为新任务重新调整
- 一致的配置更易维护

---

## 调试技巧:如何知道 r 是否合适?

### 信号 1: 训练曲线

**r 太小**:
- 训练损失下降缓慢
- 验证性能达不到预期
- 训练和验证损失都偏高

**r 合适**:
- 训练损失平滑下降
- 验证性能良好
- 训练和验证损失接近

**r 太大**:
- 训练损失很低,验证损失较高
- 明显的过拟合
- 训练不稳定

### 信号 2: 性能 vs r 曲线

绘制性能随 r 的变化:

```
性能
 ^
 |     *****  (饱和区)
 |   **
 | **
 |*
 +----------> r
   1 2 4 8 16
```

**理想曲线**:
- 快速上升后饱和
- 饱和点就是最优 r

**问题曲线**:
- 持续上升:可能需要更大的 r(但要小心过拟合)
- 先升后降:出现了过拟合

### 信号 3: 奇异值分布

如果有访问权限,检查训练后的 $\Delta W = BA$ 的奇异值:

```python
import torch

# 计算奇异值
delta_W = B @ A
U, S, V = torch.svd(delta_W)

# 绘制奇异值
import matplotlib.pyplot as plt
plt.plot(S.cpu().numpy())
plt.yscale('log')
plt.xlabel('Index')
plt.ylabel('Singular Value')
```

**r 合适**: 前几个奇异值明显大,后面快速衰减
**r 太大**: 后面的奇异值也比较大(噪声)
**r 太小**: 只有 1-2 个大奇异值

---

## 论文的超参数设置总结

### RoBERTa (Table 9)

| 模型 | 任务 | r | 性能 |
|-----|-----|---|------|
| RoBERTa-base | MNLI | 8 | 87.5% |
| RoBERTa-large | MNLI | 8 | 90.6% |

### DeBERTa (Table 10)

| 模型 | 任务 | r | 性能 |
|-----|-----|---|------|
| DeBERTa-XXL (1.5B) | MNLI | 8 | 91.9% |

### GPT-2 (Table 11)

| 模型 | 任务 | r | 性能 |
|-----|-----|---|------|
| GPT-2 Medium | E2E NLG | 4 | 70.4 BLEU |
| GPT-2 Large | E2E NLG | 4 | 70.4 BLEU |

### GPT-3 (Table 12)

| 模型 | 任务 | r | 性能 |
|-----|-----|---|------|
| GPT-3 175B | WikiSQL | 2 | 73.4% |
| GPT-3 175B | MultiNLI | 8 | 91.7% |
| GPT-3 175B | SAMSum | 8 | 53.8 R1 |

**模式**: 绝大多数情况下 r ∈ [2, 8]

---

## 实用决策树

```
开始
 |
 v
是否有充足的计算资源做搜索?
 |
 ├─ 是 -> 网格搜索 r ∈ [1,2,4,8], 选最优
 |
 └─ 否
     |
     v
    任务是否非常简单? (如二分类)
     |
     ├─ 是 -> 选 r=2
     |
     └─ 否
         |
         v
        模型是否 > 100B?
         |
         ├─ 是 -> 选 r=2
         |
         └─ 否 -> 选 r=4 或 r=8
```

---

## 总结

### 快速指南

| 情况 | 推荐 r |
|-----|--------|
| **默认选择** | **r=4 或 r=8** |
| GPT-3 级别大模型 | r=1 或 r=2 |
| 简单分类任务 | r=2 |
| 生成任务 | r=4 或 r=8 |
| 小数据集 (<1k) | r=1 或 r=2 |
| 资源受限 | r=1 |
| 不确定 | 从 r=4 开始 |

### 关键要点

1. **r 通常很小**: 1-8 覆盖大多数情况
2. **从小开始**: r=1 或 r=2,逐步增大
3. **任务相关**: 复杂任务需要稍大的 r
4. **模型相关**: 大模型需要的 r 反而更小
5. **过大有害**: r 太大会过拟合和引入噪声
6. **实验验证**: 如果可能,用验证集选择

### 论文的核心发现

> **For GPT-3 175B, we show that a very low rank (i.e., r can be one or two) suffices even when the full rank is as high as 12,288.**

这是 LoRA 最令人惊讶和最有价值的发现!
