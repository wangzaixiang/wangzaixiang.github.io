# Q11: OpenAI API çš„æ¶ˆæ¯ç»“æ„ä¸ KV Cache ä¼˜åŒ–

## é—®é¢˜
OpenAI çš„ API åœ¨å¤šè½®å¯¹è¯æ—¶ä¼šä¸æ–­å åŠ å†å²æ¶ˆæ¯ï¼š
```json
{
  "messages": [
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": "Hello!"},
    {"role": "assistant", "content": "Hi! How can I help?"},
    {"role": "user", "content": "What's the weather?"}
  ]
}
```

åŸºäº KV Cache æœºåˆ¶ï¼Œæ–°çš„ä¼šè¯æ˜¯å¦åªéœ€è¦å¤„ç†æ–°å¢çš„ message éƒ¨åˆ†çš„ tokenï¼Ÿ

## å›ç­”

**ç®€çŸ­ç­”æ¡ˆï¼šç†è®ºä¸Šå¯ä»¥ï¼Œä½†å®é™…æƒ…å†µæ›´å¤æ‚ã€‚**

è®©æˆ‘è¯¦ç»†åˆ†æ OpenAI API çš„å®ç°æœºåˆ¶å’Œ KV Cache çš„åº”ç”¨ç­–ç•¥ã€‚

---

## ä¸€ã€ç†æƒ³æƒ…å†µï¼šå®Œå…¨å¢é‡è®¡ç®—

### **ç†è®ºæ¨¡å‹**

```python
# ç¬¬ 1 è½®å¯¹è¯
messages_1 = [
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": "Hello!"}
]
# è®¡ç®—: system (50 tokens) + user (5 tokens) = 55 tokens
# ç”Ÿæˆ: "Hi! How can I help?" (10 tokens)
# ç¼“å­˜: kv_cache_1 (åŒ…å« 55 tokens çš„ K, V)

# ç¬¬ 2 è½®å¯¹è¯
messages_2 = [
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": "Hello!"},
    {"role": "assistant", "content": "Hi! How can I help?"},
    {"role": "user", "content": "What's the weather?"}
]
# ç†æƒ³æƒ…å†µ: åªè®¡ç®—æ–°å¢éƒ¨åˆ†
# æ–°å¢: assistant (10 tokens) + user (10 tokens) = 20 tokens
# å¤ç”¨: kv_cache_1 (55 tokens)
# æ€»è®¡ç®—é‡: 20 tokens (è€Œé 75 tokens)
```

**æ•°å­¦è¡¨ç¤º**ï¼š

ç¬¬ $n$ è½®å¯¹è¯ï¼š
$$\text{Tokens}_{\text{computed}} = \text{Tokens}_{\text{new}}$$

è€Œéï¼š
$$\text{Tokens}_{\text{computed}} = \sum_{i=1}^{n} \text{Tokens}_i$$

---

## äºŒã€å®é™…æŒ‘æˆ˜ä¸è§£å†³æ–¹æ¡ˆ

### **æŒ‘æˆ˜ 1ï¼šå®¢æˆ·ç«¯æ— æ³•æŒæœ‰ KV Cache**

#### **API äº¤äº’æ¨¡å‹**

```
å®¢æˆ·ç«¯                         æœåŠ¡ç«¯
  â†“
å‘é€å®Œæ•´æ¶ˆæ¯åˆ—è¡¨  â”€â”€â”€â”€â”€â”€â”€â”€â†’   æ¥æ”¶è¯·æ±‚
                              â†“
                           éœ€è¦è¯†åˆ«å“ªäº›æ˜¯"æ–°çš„"
                              â†“
                           ç†æƒ³: åªè®¡ç®—æ–°å¢éƒ¨åˆ†
                              â†“
è¿”å›å“åº”        â†â”€â”€â”€â”€â”€â”€â”€â”€   ç”Ÿæˆå›å¤
```

**é—®é¢˜**ï¼š
- å®¢æˆ·ç«¯æ¯æ¬¡å‘é€**å®Œæ•´**çš„æ¶ˆæ¯å†å²
- æœåŠ¡ç«¯æ¥æ”¶åˆ°çš„æ˜¯å®Œæ•´åˆ—è¡¨ï¼Œæ— æ³•ç›´æ¥åŒºåˆ†"æ–°å¢"å’Œ"å†å²"
- KV Cache å­˜å‚¨åœ¨æœåŠ¡ç«¯ï¼Œéœ€è¦æŸç§æœºåˆ¶å…³è”

---

### **æŒ‘æˆ˜ 2ï¼šå¯¹è¯çŠ¶æ€çš„æŒä¹…åŒ–**

#### **æ— çŠ¶æ€ API çš„å›°å¢ƒ**

ä¼ ç»Ÿçš„ REST API æ˜¯æ— çŠ¶æ€çš„ï¼š
```python
# æ¯æ¬¡è¯·æ±‚éƒ½æ˜¯ç‹¬ç«‹çš„
request_1 = POST /v1/chat/completions
request_2 = POST /v1/chat/completions  # ä¸çŸ¥é“ä¸ request_1 çš„å…³ç³»
```

**å¦‚ä½•å…³è”å¯¹è¯ï¼Ÿ**
- æ²¡æœ‰å†…ç½®çš„ `conversation_id`
- æœåŠ¡ç«¯ä¸çŸ¥é“è¿™æ˜¯"åŒä¸€ä¸ªå¯¹è¯çš„ç¬¬ 2 è½®"

---

### **è§£å†³æ–¹æ¡ˆ 1ï¼šPrompt Cachingï¼ˆæ˜ç¡®æ”¯æŒï¼‰**

OpenAI å’Œ Anthropic éƒ½å¼•å…¥äº† **Prompt Caching** åŠŸèƒ½ã€‚

#### **Anthropic Claude çš„å®ç°**

```json
{
  "model": "claude-3-5-sonnet-20241022",
  "messages": [
    {
      "role": "user",
      "content": [
        {
          "type": "text",
          "text": "You are an AI assistant...",
          "cache_control": {"type": "ephemeral"}  // æ ‡è®°ä¸ºå¯ç¼“å­˜
        },
        {
          "type": "text",
          "text": "Here is the full text of a complex document..."  ,
          "cache_control": {"type": "ephemeral"}  // æ ‡è®°ä¸ºå¯ç¼“å­˜
        },
        {
          "type": "text",
          "text": "What is the main topic?"  // ä¸ç¼“å­˜
        }
      ]
    }
  ]
}
```

**æœºåˆ¶**ï¼š
1. **å®¢æˆ·ç«¯æ ‡è®°**å“ªäº›å†…å®¹å¯ä»¥ç¼“å­˜
2. **æœåŠ¡ç«¯è®¡ç®— Hash**ï¼šå¯¹å¯ç¼“å­˜å†…å®¹ç”ŸæˆæŒ‡çº¹
3. **Cache Hit**ï¼šå¦‚æœ Hash åŒ¹é…ï¼Œç›´æ¥ä½¿ç”¨ç¼“å­˜çš„ KV
4. **Cache Miss**ï¼šè®¡ç®—å¹¶ç¼“å­˜

**æ•ˆæœ**ï¼ˆAnthropic æ•°æ®ï¼‰ï¼š
- ç¼“å­˜å‘½ä¸­ï¼šå»¶è¿Ÿé™ä½ **85%**ï¼Œæˆæœ¬é™ä½ **90%**
- ç¼“å­˜ TTLï¼š5 åˆ†é’Ÿ

#### **OpenAI çš„ Prompt Cachingï¼ˆ2024 å¹´æ¨å‡ºï¼‰**

```json
{
  "model": "gpt-4o",
  "messages": [
    {
      "role": "system",
      "content": "You are a helpful assistant."
      // OpenAI è‡ªåŠ¨è¯†åˆ« system message å¹¶ç¼“å­˜
    },
    {
      "role": "user",
      "content": "What's 2+2?"
    }
  ]
}
```

**è‡ªåŠ¨ç¼“å­˜ç­–ç•¥**ï¼š
- **System Message** è‡ªåŠ¨ç¼“å­˜
- **é•¿å‰ç¼€** è‡ªåŠ¨æ£€æµ‹å’Œç¼“å­˜ï¼ˆ> 1024 tokensï¼‰
- å®¢æˆ·ç«¯æ— éœ€æ˜¾å¼æ ‡è®°

---

### **è§£å†³æ–¹æ¡ˆ 2ï¼šå†…å®¹å¯»å€ç¼“å­˜ï¼ˆContent-Addressable Cacheï¼‰**

#### **æ ¸å¿ƒæ€æƒ³**

ä¸ä¾èµ– `conversation_id`ï¼Œè€Œæ˜¯åŸºäº**å†…å®¹æœ¬èº«**æ¥ç´¢å¼•ç¼“å­˜ã€‚

```python
class ContentAddressableCache:
    def __init__(self):
        self.cache = {}  # {content_hash: kv_cache}

    def get_cache(self, messages):
        """åŸºäºæ¶ˆæ¯å†…å®¹è·å–ç¼“å­˜"""
        # 1. è®¡ç®—æ¶ˆæ¯çš„ Hash
        cache_key = self.compute_hash(messages)

        # 2. æŸ¥æ‰¾ç¼“å­˜
        if cache_key in self.cache:
            return self.cache[cache_key]
        else:
            return None

    def compute_hash(self, messages):
        """è®¡ç®—æ¶ˆæ¯çš„å†…å®¹ Hash"""
        # æ–¹æ³• 1: ç®€å•æ‹¼æ¥
        content = json.dumps(messages, sort_keys=True)
        return hashlib.sha256(content.encode()).hexdigest()

        # æ–¹æ³• 2: åªå¯¹å‰ç¼€ Hashï¼ˆæ›´çµæ´»ï¼‰
        # å¦‚æœå‰ N æ¡æ¶ˆæ¯ç›¸åŒï¼Œå¯ä»¥å¤ç”¨

    def store_cache(self, messages, kv_cache):
        """å­˜å‚¨ç¼“å­˜"""
        cache_key = self.compute_hash(messages)
        self.cache[cache_key] = kv_cache
```

#### **å‰ç¼€åŒ¹é…ä¼˜åŒ–**

```python
def find_longest_prefix(self, new_messages):
    """æ‰¾åˆ°æœ€é•¿çš„å·²ç¼“å­˜å‰ç¼€"""
    for prefix_len in range(len(new_messages), 0, -1):
        prefix = new_messages[:prefix_len]
        cache_key = self.compute_hash(prefix)
        if cache_key in self.cache:
            return self.cache[cache_key], prefix_len
    return None, 0

# ç¤ºä¾‹
messages_round_1 = [
    {"role": "system", "content": "You are..."},
    {"role": "user", "content": "Hello"}
]
# è®¡ç®—å¹¶ç¼“å­˜ kv_1

messages_round_2 = [
    {"role": "system", "content": "You are..."},
    {"role": "user", "content": "Hello"},
    {"role": "assistant", "content": "Hi!"},
    {"role": "user", "content": "How are you?"}
]
# å‰ç¼€åŒ¹é…: å‰ 2 æ¡æ¶ˆæ¯åŒ¹é… â†’ å¤ç”¨ kv_1
# åªéœ€è®¡ç®—: assistant + user (æ–°å¢éƒ¨åˆ†)
```

---

### **è§£å†³æ–¹æ¡ˆ 3ï¼šRadix Tree Cacheï¼ˆvLLM çš„æ–¹æ¡ˆï¼‰**

#### **æ•°æ®ç»“æ„**

ä½¿ç”¨ **Radix Tree**ï¼ˆåŸºæ•°æ ‘ï¼‰å­˜å‚¨ KV Cacheï¼Œæ”¯æŒå‰ç¼€å…±äº«ã€‚

```
                    Root
                     |
              [system message]
                /          \
        [user: "Hello"]   [user: "Hi"]
           /        \
    [asst: "Hi!"] [asst: "Hello!"]
         |
    [user: "Weather?"]
```

**ç‰¹ç‚¹**ï¼š
- è‡ªåŠ¨è¯†åˆ«å…±åŒå‰ç¼€
- æ ‘çš„æ¯ä¸ªèŠ‚ç‚¹å­˜å‚¨å¯¹åº” token çš„ KV Cache
- å¤šä¸ªå¯¹è¯å¯ä»¥å…±äº«ç›¸åŒçš„å‰ç¼€éƒ¨åˆ†

#### **å®ç°ç¤ºä¾‹**

```python
class RadixTreeCache:
    class Node:
        def __init__(self):
            self.kv_cache = None  # è¯¥èŠ‚ç‚¹çš„ KV Cache
            self.children = {}    # token_id -> Node

    def __init__(self):
        self.root = self.Node()

    def find_prefix(self, tokens):
        """æŸ¥æ‰¾æœ€é•¿å‰ç¼€åŒ¹é…"""
        node = self.root
        matched_len = 0

        for i, token in enumerate(tokens):
            if token in node.children:
                node = node.children[token]
                matched_len = i + 1
            else:
                break

        return node, matched_len

    def insert(self, tokens, kv_cache):
        """æ’å…¥æ–°çš„ token åºåˆ—åŠå…¶ KV Cache"""
        node = self.root

        for i, token in enumerate(tokens):
            if token not in node.children:
                node.children[token] = self.Node()
            node = node.children[token]
            node.kv_cache = kv_cache[i]  # å­˜å‚¨è¯¥ä½ç½®çš„ KV
```

**åº”ç”¨**ï¼š
- **vLLM** ä½¿ç”¨ Radix Tree ç®¡ç†æ‰€æœ‰è¯·æ±‚çš„ KV Cache
- è‡ªåŠ¨è¯†åˆ«å’Œå¤ç”¨å…±äº«å‰ç¼€
- æ”¯æŒ Batch æ¨ç†

---

## ä¸‰ã€OpenAI çš„å®é™…å®ç°ï¼ˆæ¨æµ‹ï¼‰

è™½ç„¶ OpenAI æœªå…¬å¼€å†…éƒ¨å®ç°ï¼Œä½†æ ¹æ®è¡Œä¸ºå’Œå…¬å¼€ä¿¡æ¯ï¼Œå¯ä»¥æ¨æµ‹ï¼š

### **ç­–ç•¥ 1ï¼šè‡ªåŠ¨ Prefix Detection**

```python
class OpenAIInference:
    def __init__(self):
        self.global_cache = ContentAddressableCache()

    def chat_completion(self, messages):
        # 1. æŸ¥æ‰¾æœ€é•¿å‰ç¼€åŒ¹é…
        cached_kv, prefix_len = self.global_cache.find_longest_prefix(messages)

        # 2. å¦‚æœæœ‰ç¼“å­˜ï¼Œåªè®¡ç®—æ–°å¢éƒ¨åˆ†
        if cached_kv:
            new_messages = messages[prefix_len:]
            new_tokens = tokenize(new_messages)
            _, final_kv = self.model(new_tokens, kv_cache=cached_kv)
        else:
            # å®Œæ•´è®¡ç®—
            all_tokens = tokenize(messages)
            _, final_kv = self.model(all_tokens)

        # 3. ç¼“å­˜ç»“æœ
        self.global_cache.store(messages, final_kv)

        # 4. ç”Ÿæˆå›å¤
        return self.model.generate(kv_cache=final_kv)
```

### **ç­–ç•¥ 2ï¼šåˆ†å±‚ç¼“å­˜**

```python
# ä¸åŒç²’åº¦çš„ç¼“å­˜
caches = {
    "system_prompts": {},      # ç³»ç»Ÿæç¤ºï¼ˆé•¿æœŸï¼‰
    "conversation_prefix": {}, # å¯¹è¯å‰ç¼€ï¼ˆä¸­æœŸï¼‰
    "recent_requests": {}      # æœ€è¿‘è¯·æ±‚ï¼ˆçŸ­æœŸï¼‰
}

# ä¼˜å…ˆçº§æŸ¥æ‰¾
def get_cache(messages):
    # 1. æ£€æŸ¥ system prompt
    system_msg = messages[0] if messages[0]["role"] == "system" else None
    if system_msg:
        kv = caches["system_prompts"].get(hash(system_msg))

    # 2. æ£€æŸ¥å¯¹è¯å‰ç¼€
    # ...

    # 3. æ£€æŸ¥æœ€è¿‘è¯·æ±‚
    # ...
```

---

## å››ã€å®¢æˆ·ç«¯ä¼˜åŒ–ç­–ç•¥

è™½ç„¶æœåŠ¡ç«¯ä¼šè‡ªåŠ¨ä¼˜åŒ–ï¼Œä½†å®¢æˆ·ç«¯ä¹Ÿå¯ä»¥å¸®åŠ©æå‡ç¼“å­˜æ•ˆç‡ã€‚

### **æœ€ä½³å®è·µ 1ï¼šä¿æŒ System Message ä¸å˜**

âŒ **ä¸å¥½çš„åšæ³•**ï¼š
```python
# æ¯æ¬¡éƒ½ä¿®æ”¹ system message
messages = [
    {"role": "system", "content": f"Current time: {datetime.now()}..."},
    {"role": "user", "content": "Hello"}
]
```
**é—®é¢˜**ï¼šæ¯æ¬¡ Hash éƒ½ä¸åŒï¼Œæ— æ³•å¤ç”¨ç¼“å­˜

âœ… **å¥½çš„åšæ³•**ï¼š
```python
# System message å›ºå®š
SYSTEM_PROMPT = "You are a helpful assistant."

messages = [
    {"role": "system", "content": SYSTEM_PROMPT},
    {"role": "user", "content": "Hello"}
]
```

---

### **æœ€ä½³å®è·µ 2ï¼šç»“æ„åŒ–åŠ¨æ€å†…å®¹**

âŒ **ä¸å¥½çš„åšæ³•**ï¼š
```python
# å°†åŠ¨æ€å†…å®¹åµŒå…¥ system message
system = f"Current user: {user_name}, Time: {timestamp}, Context: {context}"
```

âœ… **å¥½çš„åšæ³•**ï¼š
```python
# System message å›ºå®šï¼ŒåŠ¨æ€å†…å®¹æ”¾åœ¨ user message
system = "You are a helpful assistant."
user_message = f"[User: {user_name}] [Time: {timestamp}]\n{question}"
```

---

### **æœ€ä½³å®è·µ 3ï¼šä½¿ç”¨ Prompt Caching APIï¼ˆå¦‚æœæ”¯æŒï¼‰**

```python
# Anthropic Claude
messages = [
    {
        "role": "user",
        "content": [
            {
                "type": "text",
                "text": LONG_DOCUMENT,
                "cache_control": {"type": "ephemeral"}  # æ ‡è®°ç¼“å­˜
            },
            {
                "type": "text",
                "text": "Summarize this."
            }
        ]
    }
]
```

---

## äº”ã€æˆæœ¬ä¸å»¶è¿Ÿçš„å½±å“

### **ç¼“å­˜å‘½ä¸­çš„æ”¶ç›Š**

å‡è®¾ä¸€ä¸ªå…¸å‹çš„å¤šè½®å¯¹è¯ï¼š

```
ç¬¬ 1 è½®:
  System: 100 tokens
  User: 20 tokens
  Total Input: 120 tokens
  Cost: 120 Ã— $0.01/1K = $0.0012

ç¬¬ 2 è½®ï¼ˆæ— ç¼“å­˜ï¼‰:
  System: 100 tokens
  User 1: 20 tokens
  Assistant 1: 50 tokens
  User 2: 20 tokens
  Total Input: 190 tokens
  Cost: 190 Ã— $0.01/1K = $0.0019

ç¬¬ 2 è½®ï¼ˆæœ‰ç¼“å­˜ï¼‰:
  Cached: 170 tokens (System + User 1 + Assistant 1)
  New: 20 tokens (User 2)
  Cost: 20 Ã— $0.01/1K + 170 Ã— $0.001/1K = $0.00037
  èŠ‚çœ: 80%
```

**OpenAI å®šä»·**ï¼ˆPrompt Cachingï¼Œ2024ï¼‰ï¼š
- æ™®é€š Inputï¼š$0.01/1K tokens
- Cached Inputï¼š$0.001/1K tokensï¼ˆ**ä¾¿å®œ 10 å€**ï¼‰

---

### **å»¶è¿Ÿæ”¹å–„**

```
æ— ç¼“å­˜:
  è®¡ç®—æ—¶é—´ âˆ Total tokens = 190 tokens
  å»¶è¿Ÿ: ~200ms

æœ‰ç¼“å­˜:
  è®¡ç®—æ—¶é—´ âˆ New tokens = 20 tokens
  å»¶è¿Ÿ: ~20ms
  æ”¹å–„: 90%
```

---

## å…­ã€å®Œæ•´ç¤ºä¾‹ä»£ç 

### **å®¢æˆ·ç«¯ä»£ç ï¼ˆPythonï¼‰**

```python
import openai

class OptimizedChatClient:
    def __init__(self, system_prompt):
        self.system_prompt = system_prompt
        self.messages = [
            {"role": "system", "content": system_prompt}
        ]

    def chat(self, user_message):
        """å‘é€æ¶ˆæ¯å¹¶æ¥æ”¶å›å¤"""
        # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
        self.messages.append({
            "role": "user",
            "content": user_message
        })

        # è°ƒç”¨ APIï¼ˆæœåŠ¡ç«¯è‡ªåŠ¨ä¼˜åŒ–ç¼“å­˜ï¼‰
        response = openai.ChatCompletion.create(
            model="gpt-4",
            messages=self.messages  # å‘é€å®Œæ•´å†å²
        )

        # ä¿å­˜åŠ©æ‰‹å›å¤
        assistant_message = response.choices[0].message.content
        self.messages.append({
            "role": "assistant",
            "content": assistant_message
        })

        return assistant_message

# ä½¿ç”¨
client = OptimizedChatClient("You are a helpful assistant.")

# ç¬¬ 1 è½®ï¼ˆè®¡ç®— system + user 1ï¼‰
reply_1 = client.chat("Hello!")

# ç¬¬ 2 è½®ï¼ˆæœåŠ¡ç«¯è‡ªåŠ¨ç¼“å­˜å‰é¢çš„å†…å®¹ï¼‰
reply_2 = client.chat("What's the weather?")
```

### **æœåŠ¡ç«¯æ¨¡æ‹Ÿä»£ç ï¼ˆç®€åŒ–ï¼‰**

```python
class CachedLLMServer:
    def __init__(self):
        self.prefix_cache = {}

    def chat_completion(self, messages):
        # 1. è®¡ç®—æ¶ˆæ¯å‰ç¼€çš„ Hash
        prefix_hashes = []
        for i in range(len(messages)):
            prefix = messages[:i+1]
            prefix_hash = self.hash_messages(prefix)
            prefix_hashes.append(prefix_hash)

        # 2. æ‰¾åˆ°æœ€é•¿ç¼“å­˜å‰ç¼€
        cached_kv = None
        start_idx = 0
        for i in reversed(range(len(prefix_hashes))):
            if prefix_hashes[i] in self.prefix_cache:
                cached_kv = self.prefix_cache[prefix_hashes[i]]
                start_idx = i + 1
                break

        # 3. åªè®¡ç®—æ–°å¢éƒ¨åˆ†
        if start_idx < len(messages):
            new_messages = messages[start_idx:]
            new_tokens = self.tokenize(new_messages)

            if cached_kv:
                _, final_kv = self.model(new_tokens, kv_cache=cached_kv)
            else:
                all_tokens = self.tokenize(messages)
                _, final_kv = self.model(all_tokens)

            # 4. ç¼“å­˜å®Œæ•´å‰ç¼€
            full_hash = prefix_hashes[-1]
            self.prefix_cache[full_hash] = final_kv

        # 5. ç”Ÿæˆå›å¤
        return self.model.generate(kv_cache=final_kv)

    def hash_messages(self, messages):
        """è®¡ç®—æ¶ˆæ¯çš„ Hash"""
        import hashlib
        content = json.dumps(messages, sort_keys=True)
        return hashlib.sha256(content.encode()).hexdigest()
```

---

## ä¸ƒã€æ€»ç»“

### **é—®é¢˜çš„ç­”æ¡ˆ**

> åŸºäº KV Cache æœºåˆ¶ï¼Œæ–°çš„ä¼šè¯æ˜¯å¦åªéœ€è¦å¤„ç†æ–°å¢çš„ message éƒ¨åˆ†çš„ tokenï¼Ÿ

**ç­”æ¡ˆ**ï¼š
1. âœ… **ç†è®ºä¸Šå®Œå…¨å¯ä»¥**ï¼šåªè®¡ç®—æ–°å¢çš„ tokens
2. âœ… **ç°ä»£ API å·²å®ç°**ï¼šOpenAIã€Anthropic éƒ½æ”¯æŒè‡ªåŠ¨ç¼“å­˜
3. âš ï¸ **éœ€è¦åŒ¹é…æœºåˆ¶**ï¼šé€šè¿‡å†…å®¹ Hash æˆ– Radix Tree è¯†åˆ«å‰ç¼€
4. ğŸ“Š **æ•ˆæœæ˜¾è‘—**ï¼šå»¶è¿Ÿé™ä½ 85%ï¼Œæˆæœ¬é™ä½ 90%

### **å…³é”®æŠ€æœ¯**

| æŠ€æœ¯ | ä½œç”¨ | åº”ç”¨ |
|------|------|------|
| **Content-Addressable Cache** | åŸºäºå†…å®¹ Hash ç´¢å¼• | è¯†åˆ«é‡å¤å‰ç¼€ |
| **Radix Tree** | æ ‘å½¢ç»“æ„å­˜å‚¨ | vLLM çš„å®ç° |
| **Prompt Caching API** | æ˜¾å¼ç¼“å­˜æ ‡è®° | Anthropic Claude |
| **è‡ªåŠ¨å‰ç¼€æ£€æµ‹** | é€æ˜ä¼˜åŒ– | OpenAI GPT-4 |

### **æœ€ä½³å®è·µ**

1. **ä¿æŒ System Message å›ºå®š**
2. **åŠ¨æ€å†…å®¹æ”¾åœ¨ User Message**
3. **ä½¿ç”¨ Prompt Caching APIï¼ˆå¦‚æœæ”¯æŒï¼‰**
4. **é¿å…åœ¨æ¯æ¬¡è¯·æ±‚ä¸­ä¿®æ”¹å†å²æ¶ˆæ¯**

### **æ€§èƒ½æ”¶ç›Š**

```
å¤šè½®å¯¹è¯ï¼ˆ10 è½®ï¼‰:
  æ— ç¼“å­˜: è®¡ç®— (100 + 200 + 300 + ... + 1000) tokens
  æœ‰ç¼“å­˜: è®¡ç®— 10 Ã— 100 tokens

  æ—¶é—´èŠ‚çœ: 95%
  æˆæœ¬èŠ‚çœ: 90%
```

---

## ç›¸å…³ä¸»é¢˜
- Anthropic Prompt Caching è¯¦è§£
- OpenAI Prompt Caching å®šä»·ç­–ç•¥
- vLLM çš„ Automatic Prefix Caching
- Radix Tree æ•°æ®ç»“æ„
- Content-Addressable Storage
- å¤šç§Ÿæˆ·åœºæ™¯çš„ Cache å…±äº«
- Cache é©±é€ç­–ç•¥ï¼ˆLRU/LFUï¼‰

